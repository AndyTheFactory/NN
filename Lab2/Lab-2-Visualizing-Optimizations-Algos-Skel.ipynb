{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing Optimization Algorithms"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Intro"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The task in this lab session is to implement a subset of the known optimization algorithms used in neural network training. \n",
        "\n",
        "Specifically, you are given a function ([The Beale Function](https://www.sfu.ca/~ssurjano/beale.html)) simulating the _loss_ space in which gradient descent with respect to the parameters needs to be performed. To simplify visualization, the function that models the loss space is defined over two parameters: $(x, y)$.\n",
        "\n",
        "Your task is to implement a method to compute the gradient of the loss function with respect to its two parameters and then use the gradient vector to implement and analyze the behavior of the following optimization methods: **SGD**, **Momentum**, **Nesterov Gradient**, **Adagrad**, **Adadelta**, **RMSProp** and **Adam**."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['savefig.dpi'] = 80\n",
        "mpl.rcParams['figure.dpi'] = 80"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "#import autograd.numpy as np\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.colors import LogNorm\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "from autograd import elementwise_grad, value_and_grad\n",
        "#from scipy.optimize import minimize\n",
        "from collections import defaultdict\n",
        "from itertools import zip_longest\n",
        "from functools import partial"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Beale function**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def beale(x, y):\n",
        "    return (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define plot limits**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "xmin, xmax, xstep = -4.5, 4.5, .2\n",
        "ymin, ymax, ystep = -4.5, 4.5, .2"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "mesh_x, mesh_y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "mesh_z = beale(mesh_x, mesh_y)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **global  minimum** of the Beale function is at $(3, 0.5)$. Let us check that it is indeed so:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "minimum = np.array([3, 0.5])\n",
        "beale(*minimum)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transpose minimum to be used for plotting:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "minimum_t = minimum.reshape(-1, 1)\n",
        "print(minimum_t.shape)\n",
        "\n",
        "beale(*minimum_t)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Visualization of the 3D Surface Plot of the Beale Function"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(8, 5))\n",
        "ax = plt.axes(projection='3d', elev=50, azim=-50)\n",
        "\n",
        "ax.plot_surface(mesh_x, mesh_y, mesh_z, norm=LogNorm(), rstride=1, cstride=1, \n",
        "                edgecolor='none', alpha=.6, cmap=plt.cm.jet)\n",
        "ax.plot(*minimum_t, beale(*minimum_t), 'r*', markersize=10)\n",
        "\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$y$')\n",
        "ax.set_zlabel('$z$')\n",
        "\n",
        "ax.set_xlim((xmin, xmax))\n",
        "ax.set_ylim((ymin, ymax))\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us plot the Gradient Vector Field. We use `autograd` to compute it and the Matplotlib `quiver` method to plot it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "dbeale_dx = elementwise_grad(beale, argnum=0)(mesh_x, mesh_y)\n",
        "dbeale_dy = elementwise_grad(beale, argnum=1)(mesh_x, mesh_y)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax.contour(mesh_x, mesh_y, mesh_z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)\n",
        "ax.quiver(mesh_x, mesh_y, mesh_x - dbeale_dx, mesh_y - dbeale_dy, alpha=.5)\n",
        "ax.plot(*minimum_t, 'r*', markersize=18)\n",
        "\n",
        "ax.set_xlabel('$x$')\n",
        "ax.set_ylabel('$y$')\n",
        "\n",
        "ax.set_xlim((xmin, xmax))\n",
        "ax.set_ylim((ymin, ymax))\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Helper functions"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "`trace_path` allows you to visualize the trace of parameter values given by each optimization method. `optimization_sol` contains a list of numpy arrays, each with 2 elements: (x, y) values for the two parameters defining the loss function."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def trace_path(optimization_sol):\n",
        "    path = np.array(optimization_sol).T\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    ax.contour(mesh_x, mesh_y, mesh_z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)\n",
        "    ax.quiver(path[0,:-1], path[1,:-1], path[0,1:]-path[0,:-1], path[1,1:]-path[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n",
        "    ax.plot(*minimum_t, 'r*', markersize=18)\n",
        "\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "\n",
        "    ax.set_xlim((xmin, xmax))\n",
        "    ax.set_ylim((ymin, ymax))\n",
        "\n",
        "    plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "`view_update_history` allows you to visualize the history of _parameter updates_ (i.e. $\\Delta \\theta$ values) produced by each optimization method. `updates` is a list of numpy arrays , each with two elements: the update values for the (x, y) parameters at a given time step."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def view_update_history(updates):\n",
        "    deltas = np.array(updates).T\n",
        "    \n",
        "    delta_x = deltas[0, :]\n",
        "    delta_y = deltas[1, :]\n",
        "    \n",
        "    xx = np.arange(0, delta_x.shape[0])\n",
        "    \n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(xx, delta_x, 'o-')\n",
        "    plt.title('Parameter update history')\n",
        "    plt.ylabel('Updates to x direction')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(xx, delta_y, 'o-')\n",
        "    plt.xlabel('Update step')\n",
        "    plt.ylabel('Updates to y direction')\n",
        "\n",
        "    plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Optimization algorithms"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set the starting point as  $(1,1.75)$ , since it will allow us to exemplify overshoot behavior in momentum based gradient descent update rules. \n",
        "\n",
        "After you complete all tasks, you can rerun your optimizers with the other starting points, to analyze cases where the update rules lead to overshooting and ending up in some local minima."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "start_tall = np.array([3., 4.])\n",
        "start_med = np.array([1., 1.75])\n",
        "start_low = np.array([1., 1.])\n",
        "\n",
        "EPOCHS = 400"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 2.1: compute the gradient of the beale function. \n",
        "The function has to return a tuple `(df_dx, df_dy)` representing the gradient vector of the Beale function in point `(x, y)`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_beale(x, y):\n",
        "    # TODO 1\n",
        "    # df_dx = .....\n",
        "    # df_dy = ....\n",
        "    df_dx = 2*( 1.5*(y-1)+ 2.25*(y**2-1) + 2.625*(y**3-1) + x*( (y-1)**2 + (y**2-1)**2 + (y**3-1)**2 ) )\n",
        "    df_dy = 2*x*( 1.5 + x*(y-1) + 2*2.25*y + 2*x*(y**3-y) + 3*2.625*y**2 + 3*x*(y**5-y**2) )\n",
        "    \n",
        "    return df_dx,df_dy"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print( grad_beale(1,11))\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify if we have computed the gradient correctly by comparing the result with the output of the `elementwise_grad` function from `autograd`:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ref_dx = elementwise_grad(beale, argnum=0)(mesh_x, mesh_y)\n",
        "ref_dy = elementwise_grad(beale, argnum=1)(mesh_x, mesh_y)\n",
        "\n",
        "df_dx, df_dy = grad_beale(mesh_x, mesh_y)\n",
        "\n",
        "np.testing.assert_array_almost_equal(ref_dx, df_dx, decimal = 6, verbose=True)\n",
        "np.testing.assert_array_almost_equal(ref_dy, df_dy, decimal = 6, verbose=True)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 2.2 Implement the SGD optimizer\n",
        "$ \\theta^{(k+1)} = \\theta^{(k)} + \\lambda \\nabla_{\\theta} J(x^{(k)}, \\theta^{(k)}) $"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def SGD_beale(start=start_med, target=minimum, lr=1e-3, epochs=EPOCHS):\n",
        "    idx = 0\n",
        "    solutions = [start]\n",
        "    updates = []\n",
        "    \n",
        "    params = start\n",
        "    while idx < epochs:\n",
        "        dx, dy = grad_beale(*params)\n",
        "        \n",
        "        ## delta_params = .... \n",
        "        delta_params=lr*np.array((dx,dy))\n",
        "        params = params - delta_params\n",
        "        \n",
        "        # we are saving the parameter values and the update deltas to plot them\n",
        "        solutions.append(params)\n",
        "        updates.append(delta_params)\n",
        "        \n",
        "        idx += 1\n",
        "        \n",
        "    return solutions, updates, np.linalg.norm(params - target)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the parameter trace, the update vectors and the final distance from the global minimum"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sgd_sol, sgd_updates, sgd_diff = SGD_beale(start=start_med)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize SGD parameter trace and update vector history."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected distance from the true minimum, when running SGD with default parameters: `1.017168167327364`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![SGD Parameter Path](img/sgd_parameter_path.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(sgd_diff)\n",
        "trace_path(sgd_sol)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "view_update_history(sgd_updates)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.3 Implement the SGD with Momentum optimizer\n",
        "\\begin{align*}\n",
        "        \\Delta\\theta_t = \\gamma \\Delta\\theta_{t-1} + \\lambda \\nabla_{\\theta} J(\\theta)\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "        \\theta^{(t + 1)} = \\theta^{(t)} - \\Delta\\theta_t\n",
        "\\end{align*}"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_beale(start=start_med, target=minimum, lr=1e-3, gamma=0.9, epochs=EPOCHS):\n",
        "    delta_params = np.zeros(2)\n",
        "    \n",
        "    idx = 0\n",
        "    solutions = [start]\n",
        "    updates = []\n",
        "    \n",
        "    params = start\n",
        "    while idx < epochs:\n",
        "        dx, dy = grad_beale(*params)\n",
        "        \n",
        "        ## TODO \n",
        "        ## delta_params = ....\n",
        "        delta_params=gamma*delta_params+lr*np.array((dx,dy))\n",
        "        params = params - delta_params\n",
        "        solutions.append(params)\n",
        "        updates.append(delta_params)\n",
        "        \n",
        "        idx += 1\n",
        "        \n",
        "    return solutions, updates, np.linalg.norm(params - target)\n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "momentum_sol, momentum_updates, momentum_diff = momentum_beale(start=start_med)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected distance from the true minimum, when running Momentum with default parameters: `0.13853887236394447`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Momentum Parameter Path](img/momentum_parameter_path.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(momentum_diff)\n",
        "trace_path(momentum_sol)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "view_update_history(momentum_updates)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 2.4 Implement Nesterov Accelerated Gradient optimizer\n",
        "NAG performs a \"look-ahead\" strategy by computing the gradient of the loss function in a point given by future estimated parameter values $\\theta - \\gamma \\Delta\\theta_{t-1}$, instead of the current $\\theta$. Since we do not have the _new_ $\\Delta \\theta_t$, we approximate the future parameter values with the previous update $\\Delta \\theta_{t-1}$\n",
        "    \n",
        "  \\begin{align*} \n",
        "        \\Delta\\theta_t = \\gamma \\Delta\\theta_{t-1} + \\lambda \\nabla_{\\theta} J(\\theta - \\gamma \\Delta\\theta_{t-1})\n",
        "  \\end{align*}\n",
        "   \n",
        "  \\begin{align*}\n",
        "        \\theta^{(t + 1)} = \\theta^{(t)} - \\Delta\\theta_{t}\n",
        "  \\end{align*}"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov_beale(start=start_med, target=minimum, lr=1e-3, gamma=0.9, epochs=EPOCHS):\n",
        "    delta_params = np.zeros(2)\n",
        "    \n",
        "    idx = 0\n",
        "    solutions = [start]\n",
        "    updates = []\n",
        "    \n",
        "    params = start\n",
        "    while idx < epochs:\n",
        "        ## TODO\n",
        "        ## delta_params = ....\n",
        "        params_future=params-gamma*delta_params\n",
        "        dx,dy=grad_beale(params_future[0],params_future[1])\n",
        "        delta_params=gamma*delta_params+lr*np.array((dx,dy))\n",
        "        params = params - delta_params\n",
        "        solutions.append(params)\n",
        "        updates.append(delta_params)\n",
        "        \n",
        "        idx += 1\n",
        "        \n",
        "    return solutions, updates, np.linalg.norm(params - target)\n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "nesterov_sol, nesterov_updates, nesterov_diff = nesterov_beale(start=start_med)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected distance from the true minimum, when running Nesterov Accelerated Gradient with default parameters: `0.13359752885046636`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Nesterov Parameter Path](img/nesterov_parameter_path.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(nesterov_diff)\n",
        "trace_path(nesterov_sol)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "view_update_history(nesterov_updates)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.5 Implement Adagrad optimizer\n",
        "Adagrad tries to adjust the learning rate for each parameter by decaying it using the history of past gradient updates.\n",
        "\n",
        "The gradient with respect to parameter $\\theta_i$ at step $t$ is written as: $g_{t,i} = \\nabla J_{\\theta}(\\theta^{(t)}_i)$.\n",
        "\n",
        "\n",
        "The Adagrad update rule is the following:\n",
        "$\\theta^{(t+1)}_i = \\theta^{(t)}_i - \\frac{\\lambda}{\\sqrt{G_{t,ii} + \\epsilon}} g_{t,i}$, \n",
        "\n",
        "where $G_t \\in \\mathbb{R}^{d \\times d}$ - diagonal matrix. Element $i,i$ on the diagonal = sum of squares of past gradients (up to step $t$) for parameter $\\theta_i$."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "def adagrad_beale(start=start_med, target=minimum, lr=1e-2, eps=1e-5, epochs=EPOCHS):\n",
        "    idx = 0\n",
        "    solutions = [start]\n",
        "    updates = []\n",
        "    \n",
        "    params = start\n",
        "    G = np.zeros((start.shape[0], start.shape[0]))\n",
        "    \n",
        "    while idx < epochs:\n",
        "        dx, dy = grad_beale(*params)\n",
        "        \n",
        "        \n",
        "        ## TODO\n",
        "        ## delta_params = ....\n",
        "        g=np.array((dx,dy))\n",
        "        G=G+np.diag(g**2)\n",
        "        coef=1.0/(np.sqrt(np.diag(G))+sys.float_info.epsilon)       \n",
        "        delta_params= lr*np.dot(np.diag(coef),g)\n",
        "        params = params - delta_params\n",
        "        solutions.append(params)\n",
        "        updates.append(delta_params)\n",
        "        \n",
        "        idx += 1\n",
        "        \n",
        "    return solutions, updates, np.linalg.norm(params - target)\n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "adagrad_sol, adagrad_updates, adagrad_diff = adagrad_beale(start=start_med, lr=1e-1)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected distance from the true minimum, when running Adagrad with default parameters: `1.580078329786008`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Adagrad Parameter Path](img/adagrad_parameter_path.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(adagrad_diff)\n",
        "trace_path(adagrad_sol)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "view_update_history(adagrad_updates)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 2.6 Implement the Adadelta optimizer\n",
        "In Adadelta we simulate a _window_ of past gradient updates by computing a decaying running average of past squared gradients.\n",
        "\n",
        "$E[g^2]_t = \\gamma E[g^2]_{t-1} + (1 - \\gamma) g^2_t, \\text{where } E[g^2]_t \\text{ is the running average at step t.}$\n",
        "<br/><br/>\n",
        "\n",
        "Adadelta also defines an exponential decay of squared parameter updates ($\\Delta \\theta$).\n",
        "\n",
        "$E[\\Delta\\theta^2]_t = \\gamma E[\\Delta\\theta^2]_{t-1} + (1 - \\gamma) \\Delta\\theta^2_t$\n",
        "<br/><br/>\n",
        "\n",
        "The Adadelta update rule is thus:\n",
        "\n",
        "\\begin{align*}\n",
        "        RMS[\\Delta\\theta]_t &= \\sqrt{E[\\Delta\\theta^2]_t + \\epsilon} \\\\\n",
        "        %\n",
        "        \\Delta\\theta_t &= \\frac{RMS[\\Delta \\theta]_{t-1}}{RMS[g]_t} g_t, \\text{where } RMS[\\Delta \\theta]_{t-1} \\text{ replaces } \\lambda \\\\\n",
        "        %\n",
        "        \\theta^{(t+1)} &= \\theta^{(t)} - \\Delta \\theta_t\n",
        "\\end{align*}"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def adadelta_beale(start=start_med, target=minimum, gamma=0.9, eps=1e-5, epochs=EPOCHS):\n",
        "    idx = 0\n",
        "    solutions = [start]\n",
        "    updates = []\n",
        "    \n",
        "    params = start\n",
        "    Etheta = np.zeros(start.shape[0])\n",
        "    Eg = np.zeros(start.shape[0])\n",
        "    \n",
        "    while idx < epochs:\n",
        "        dx, dy = grad_beale(*params)\n",
        "        grad = np.array([dx, dy])\n",
        "        \n",
        "        ## Update the parameter holding the decayed average of previous gradient-square values\n",
        "        ## Eg = ...\n",
        "        ## Compute RMS[g] = ....\n",
        "        Eg=gamma*Eg+(1-gamma)*(grad**2)\n",
        "        RMSgt=np.sqrt(Eg+eps)\n",
        "        \n",
        "        ## Compute RMS[theta]\n",
        "        ## RMStheta = .... \n",
        "        RMStheta=np.sqrt(Etheta+eps)\n",
        "        \n",
        "        ## Compute delta_params\n",
        "        ## delta_params = ....\n",
        "        delta_params=(RMStheta/RMSgt)*grad\n",
        "\n",
        "        params = params - delta_params\n",
        "        solutions.append(params)\n",
        "        updates.append(delta_params)\n",
        "        \n",
        "        ## Compute Etheta \n",
        "        ## Etheta = ....\n",
        "        Etheta=gamma*Etheta+(1-gamma)*(delta_params**2)\n",
        "        idx += 1\n",
        "        \n",
        "    return solutions, updates, np.linalg.norm(params - target)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "adadelta_sol, adadelta_updates, adadelta_diff = adadelta_beale(start=start_med)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected distance from the true minimum, when running Adadelta with default parameters: `0.3303773602709788`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Adadelta Parameter Path](img/adadelta_parameter_path.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(adadelta_diff)\n",
        "trace_path(adadelta_sol)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "view_update_history(adadelta_updates)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 2.7 Implement RMSProp optimizer\n",
        "RMSProp works almost the same as Adadelta, but it reintroduces a global learning rate $\\lambda$, instead of the $RMS[\\Delta \\theta]$ term.\n",
        "\n",
        "The RMSProp update rule is:\n",
        "\\begin{align*}\n",
        "        E[g^2]_t &= 0.9 E[g^2]_{t-1} + 0.1 g^2_t\\\\\n",
        "        \\theta^{(t+1)} &= \\theta^{(t)} - \\frac{\\lambda}{\\sqrt{E[g^2]_t + \\epsilon}} g_t\n",
        "\\end{align*}"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop_beale(start=start_med, target=minimum, lr=1e-3, gamma=0.9, eps=1e-5, epochs=EPOCHS):\n",
        "    idx = 0\n",
        "    solutions = [start]\n",
        "    updates = []\n",
        "    \n",
        "    params = start\n",
        "    Eg = np.zeros(start.shape[0])\n",
        "    \n",
        "    while idx < epochs:\n",
        "        dx, dy = grad_beale(*params)\n",
        "        grad = np.array([dx, dy])\n",
        "        \n",
        "        ## TODO\n",
        "        ## Ef = ....\n",
        "        \n",
        "        ## TODO\n",
        "        ## RMSg = ...\n",
        "        \n",
        "        ## TODO\n",
        "        ## delta_params = ....\n",
        "        \n",
        "        Eg=gamma*Eg+(1-gamma)*(grad**2)\n",
        "        RMSgt=np.sqrt(Eg+eps)\n",
        "        \n",
        "        \n",
        "        ## Compute delta_params\n",
        "        ## delta_params = ....\n",
        "        delta_params=(lr/RMSgt)*grad\n",
        "        \n",
        "\n",
        "        params = params - delta_params\n",
        "        solutions.append(params)\n",
        "        updates.append(delta_params)\n",
        "        \n",
        "        idx += 1\n",
        "        \n",
        "    return solutions, updates, np.linalg.norm(params - target)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "rmsprop_sol, rmsprop_updates, rmsprop_diff = rmsprop_beale(start=start_med, lr=1e-2)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected distance from the true minimum, when running RMSPropwith default parameters: `0.09687844841688657`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![RMSProp Parameter Path](img/rmsprop_parameter_path.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(rmsprop_diff)\n",
        "trace_path(rmsprop_sol)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "view_update_history(rmsprop_updates)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO 2.8 Implement Adam optimizer\n",
        "\n",
        "\n",
        "For Adam we store two averages:\n",
        "    - store exponentially decaying average of _squared gradients_ $v_t$ (second moment - variance)\n",
        "    - store exponentially decaying average of _past gradients_ $m_t$ (first moment - mean), similar to momentum\n",
        "    \n",
        "\\begin{align*}\n",
        "        m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\\n",
        "        %\n",
        "        v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) g^2_t\n",
        "\\end{align*}\n",
        "\n",
        "Perform bias correction to avoid having $m_t$ and $v_t$ too close to 0.\n",
        "\n",
        "\\begin{align*}\n",
        "        \\hat{m}_t &= \\frac{m_t}{1-\\beta_1^t}\\\\\n",
        "        \\hat{v}_t &= \\frac{v_t}{1-\\beta_2^t}\n",
        "\\end{align*}\n",
        "\n",
        "The Adam update rule is then:\n",
        "\\begin{align*}\n",
        "    \\theta^{(t+1)} = \\theta^{(t)} - \\frac{\\lambda}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n",
        "\\end{align*}"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def adam_beale(start=start_med, target=minimum, lr=1e-3, beta1=0.9, beta2=0.9, eps=1e-5, epochs=EPOCHS):\n",
        "    idx = 0\n",
        "    solutions = [start]\n",
        "    updates = []\n",
        "    params = start\n",
        "    \n",
        "    m = np.zeros(start.shape[0])\n",
        "    v = np.zeros(start.shape[0])\n",
        "    \n",
        "    beta1_norm = 1.0\n",
        "    beta2_norm = 1.0\n",
        "    \n",
        "    while idx < epochs:\n",
        "        dx, dy = grad_beale(*params)\n",
        "        grad = np.array([dx, dy])\n",
        "        \n",
        "        ## TODO:\n",
        "        ## delta_params = ...\n",
        "        \n",
        "        m=beta1*m+(1-beta1)*grad\n",
        "        v=beta2*v+(1-beta2)*(grad**2)\n",
        "        \n",
        "        beta1_norm=beta1_norm*beta1\n",
        "        beta2_norm=beta2_norm*beta2\n",
        "        \n",
        "        m_hat=m/(1-beta1_norm)\n",
        "        v_hat=v/(1-beta2_norm)\n",
        "        \n",
        "        delta_params=lr*m_hat/(np.sqrt(v_hat)+eps)\n",
        "        \n",
        "        params = params - delta_params\n",
        "        solutions.append(params)\n",
        "        updates.append(delta_params)\n",
        "        \n",
        "        idx += 1\n",
        "        \n",
        "    return solutions, updates, np.linalg.norm(params - target)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "adam_sol, adam_updates, adam_diff = adam_beale(start=start_med, lr = 1e-2)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected distance from the true minimum, when running Adam with default parameters: `0.03735099816997893`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Adam Parameter Path](img/adam_parameter_path.png)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(adam_diff)\n",
        "trace_path(adam_sol)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "view_update_history(adam_updates)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Comparative visualization"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compare the behavior of the implemented optimizers by animating the traces of parameter values."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "METHODS = [\n",
        "    'sgd', \n",
        "    'momentum', \n",
        "    'nesterov', \n",
        "    'adagrad', \n",
        "    'adadelta', \n",
        "    'rmsprop', \n",
        "    'adam'\n",
        "]\n",
        "\n",
        "class TrajectoryAnimation(animation.FuncAnimation):\n",
        "    \n",
        "    def __init__(self, paths_dict, fig=None, ax=None, frames=None, \n",
        "                 interval=60, repeat_delay=5, blit=True, **kwargs):\n",
        "        \n",
        "        self.paths_dict = paths_dict\n",
        "        \n",
        "        if fig is None:\n",
        "            if ax is None:\n",
        "                fig, ax = plt.subplots()\n",
        "            else:\n",
        "                fig = ax.get_figure()\n",
        "        else:\n",
        "            if ax is None:\n",
        "                ax = fig.gca()\n",
        "\n",
        "        self.fig = fig\n",
        "        self.ax = ax\n",
        "        \n",
        "        self.__set_paths_and_labels()\n",
        "\n",
        "        if frames is None:\n",
        "            frames = max(path.shape[1] for path in self.paths)\n",
        "  \n",
        "        self.lines = [ax.plot([], [], label=label, lw=2)[0] \n",
        "                      for _, label in zip_longest(self.paths, self.labels)]\n",
        "        self.points = [ax.plot([], [], 'o', color=line.get_color())[0] \n",
        "                       for line in self.lines]\n",
        "\n",
        "        super(TrajectoryAnimation, self).__init__(fig, self.animate, init_func=self.init_anim,\n",
        "                                                  frames=frames, interval=interval, blit=blit,\n",
        "                                                  repeat_delay=repeat_delay, **kwargs)\n",
        "\n",
        "    def __set_paths_and_labels(self):\n",
        "        self.paths = []\n",
        "        self.labels = []\n",
        "        \n",
        "        for m in METHODS:\n",
        "            if m in self.paths_dict:\n",
        "                self.labels.append(m)\n",
        "                self.paths.append( np.array(self.paths_dict[m]).T )\n",
        "        \n",
        "    \n",
        "    def init_anim(self):\n",
        "        for line, point in zip(self.lines, self.points):\n",
        "            line.set_data([], [])\n",
        "            point.set_data([], [])\n",
        "        return self.lines + self.points\n",
        "\n",
        "    def animate(self, i):\n",
        "        for line, point, path in zip(self.lines, self.points, self.paths):\n",
        "            line.set_data(*path[::,:i])\n",
        "            point.set_data(*path[::,i-1:i])\n",
        "        return self.lines + self.points\n",
        "\n",
        "\n",
        "def animate_paths(paths_dict):\n",
        "    # define figure and axes\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.contour(mesh_x, mesh_y, mesh_z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap=plt.cm.jet)\n",
        "    ax.plot(*minimum_t, 'r*', markersize=18)\n",
        "    \n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "\n",
        "    ax.set_xlim((xmin, xmax))\n",
        "    ax.set_ylim((ymin, ymax))\n",
        "    \n",
        "    anim = TrajectoryAnimation(paths_dict, ax = ax)\n",
        "    \n",
        "    ax.legend(loc='upper left')\n",
        "\n",
        "    return anim"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# define paths_dict\n",
        "paths_dict = {\n",
        "    'sgd': sgd_sol[::2],\n",
        "    'momentum': momentum_sol[::5],\n",
        "    'nesterov': nesterov_sol[::5],\n",
        "    'adagrad': adagrad_sol[::5],\n",
        "    'adadelta': adadelta_sol[::5],\n",
        "    'rmsprop': rmsprop_sol[::5],\n",
        "    'adam': adam_sol[::5]\n",
        "}\n",
        "\n",
        "anim = animate_paths(paths_dict)\n",
        "\n",
        "HTML(anim.to_html5_video())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}